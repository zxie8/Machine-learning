{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from random import seed\n",
    "from random import random\n",
    "from random import randrange\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f6ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "In general, LR's idea is to make prediction, and improve weights and bias from the cost function that is calculated from prediction / GroundTruth\n",
    "'''\n",
    "\n",
    "class LogisticRegression():\n",
    "    def __init__(self, X, learning_rate = 0.1, num_iters = 10000):\n",
    "        self.lr = learning_rate\n",
    "        self.num_iters = num_iters\n",
    "        #m samples and n features\n",
    "        self.m, self.n = X.shape\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        self.weights = np.zeros((self.n, 1))\n",
    "        self.bias = 0\n",
    "        \n",
    "        for it in range(self.num_iters + 1):\n",
    "            #calculate hypothesis\n",
    "            y_predict = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "            #calculate cost\n",
    "            cost = -1/self.m * np.sum(y*np.log(y_predict) + (1 - y) * np.log(1 - y_predict))\n",
    "            #backprop\n",
    "            dw = 1/self.m * np.dot(X.T, (y_predict - y))\n",
    "            db = 1/self.m * np.sum(y_predict - y)\n",
    "            \n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "            \n",
    "            if it % 1000 == 0:\n",
    "                print(f'Cost after iteration {it}: {cost}')\n",
    "        return self.weights, self.bias\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_predict = self.sigmoid(np.dot(X, self.weights)) + self.bias\n",
    "        y_predict_labels = y_predict > 0.5\n",
    "        return y_predict_labels\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "def plot_data(dataset,color = 'r'):\n",
    "    d = collections.defaultdict(list)\n",
    "    for x, y, data_type in dataset:\n",
    "        d[data_type].append([x,y])\n",
    "    for key in d.keys():\n",
    "        data = np.transpose(np.array(d[key]))\n",
    "        plt.scatter(data[0],data[1], label = key)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ea8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's test it out.\n",
    "'''\n",
    "if __name__ =='__main__':\n",
    "    np.random.seed(1)\n",
    "    #create test data\n",
    "    x1_base, y1_base, x2_base, y2_base = 0, 0, 2, 2\n",
    "    x1_rand, y1_rand, x2_rand, y2_rand = 1, 1, 1, 1\n",
    "    type_1data = [[x1_base + (random() - 0.5) * x1_rand, y1_base + (random() - 0.5) * y1_rand, 0] for _ in range(100)]\n",
    "    type_2data = [[x2_base + (random() - 0.5) * x2_rand, y2_base + (random() - 0.5) * y2_rand, 1] for _ in range(100)]\n",
    "    dataset =  np.array(type_1data + type_2data)\n",
    "    shuffle(dataset)\n",
    "    X = dataset[:,:2]\n",
    "    y = dataset[:,2]\n",
    "    y = y[:, np.newaxis]\n",
    "    logreg = LogisticRegression(X)\n",
    "    w, b = logreg.train(X, y)\n",
    "    y_predict = logreg.predict(X)\n",
    "    print(f'Accuracy: {np.sum(y==y_predict) / X.shape[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
